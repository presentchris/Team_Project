{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e708abc-5c2b-44d3-b687-3ca91cdbdc92",
   "metadata": {},
   "source": [
    "# Visual Question Answering and Image Captioning using BLIP and OpenVINO\n",
    "\n",
    "Humans perceive the world through vision and language. A longtime goal of AI is to build intelligent agents that can understand the world through vision and language inputs to communicate with humans through natural language. In order to achieve this goal, vision-language pre-training has emerged as an effective approach, where deep neural network models are pre-trained on large scale image-text datasets to improve performance on downstream vision-language tasks, such as image-text retrieval, image captioning, and visual question answering.\n",
    "\n",
    "[BLIP](https://github.com/salesforce/BLIP) is a language-image pre-training framework for unified vision-language understanding and generation. BLIP achieves state-of-the-art results on a wide range of vision-language tasks. This tutorial demonstrates how to use BLIP for visual question answering and image captioning.\n",
    "\n",
    "The tutorial consists of the following parts:\n",
    "\n",
    "1. Instantiate a BLIP model.\n",
    "2. Convert the BLIP model to OpenVINO IR.\n",
    "3. Run visual question answering and image captioning with OpenVINO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625e4c35",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "- [Background](#Background)\n",
    "    - [Image Captioning](#Image-Captioning)\n",
    "    - [Visual Question Answering](#Visual-Question-Answering)\n",
    "- [Instantiate Model](#Instantiate-Model)\n",
    "- [Convert Models to OpenVINO IR](#Convert-Models-to-OpenVINO-IR)\n",
    "    - [Vision Model](#Vision-Model)\n",
    "    - [Text Encoder](#Text-Encoder)\n",
    "    - [Text Decoder](#Text-Decoder)\n",
    "- [Run OpenVINO Model](#Run-OpenVINO-Model)\n",
    "    - [Prepare Inference Pipeline](#Prepare-Inference-Pipeline)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Image Captioning](#Image-Captioning)\n",
    "    - [Question Answering](#Question-Answering)\n",
    "- [Interactive demo](#Interactive-demo)\n",
    "- [Next steps](#Next-steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91e1ce-6867-493b-b7e5-093013f967f2",
   "metadata": {},
   "source": [
    "## Background\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Visual language processing is a branch of artificial intelligence that focuses on creating algorithms designed to enable computers to more accurately understand images and their content.\n",
    "\n",
    "Popular tasks include:\n",
    "\n",
    "* **Text to Image Retrieval** - a semantic task that aims to find the most relevant image for a given text description.\n",
    "* **Image Captioning** - a semantic task that aims to provide a text description for image content.\n",
    "* **Visual Question Answering** - a semantic task that aims to answer questions based on image content.\n",
    "\n",
    "As shown in the diagram below, these three tasks differ in the input provided to the AI system. For text-to-image retrieval, you have a predefined gallery of images for search and a user-requested text description (query). Image captioning can be represented as a particular case of visual question answering, where you have a predefined question \"What is in the picture?\" and various images provided by a user. For visual question answering, both the text-based question and image context are variables requested by a user.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/221755717-a5b51b7e-523c-461f-b30c-4edbfaf9a134.png)\n",
    "\n",
    "This notebook does not focus on Text to Image retrieval. Instead, it considers Image Captioning and Visual Question Answering.\n",
    "\n",
    "### Image Captioning\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Image Captioning is the task of describing the content of an image in words. This task lies at the intersection of computer vision and natural language processing. Most image captioning systems use an encoder-decoder framework, where an input image is encoded into an intermediate representation of the information in the image, and then decoded into a descriptive text sequence.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/221640847-1868117c-aac0-4806-99a4-34f218e98bb8.png)\n",
    "\n",
    "### Visual Question Answering\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Visual Question Answering (VQA) is the task of answering text-based questions about image content.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/221641984-3c6d8b2f-dd0d-4302-a4d8-0f8564fca772.png)\n",
    "\n",
    "For a better understanding of how VQA works, let us consider a traditional NLP task like Question Answering, which aims to retrieve the answer to a question from a given text input. Typically, a question answering pipeline consists of three steps:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/221760881-378f1ea8-eadc-4610-aff0-69ecabf62fff.png)\n",
    "\n",
    "1. Question analysis - analysis of provided question in natural language form to understand the object in the question and additional context. For example, if you have a question like \"How many bridges in Paris?\", question words *\"how many\"* gives a hint that the answer is more likely to be a number, *\"bridges\"* is the target object of the question and *\" in Paris\"* serves as additional context for the search.\n",
    "2. Build query for search - use analyzed results to formalize query for finding the most relevant information.\n",
    "3. Perform a search in the knowledge base - send the query to a knowledge base, typically provided text documents or databases serve as a source of knowledge.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/222094861-3cafdf9f-d700-4741-b6c5-fb09c1a4da9a.png)\n",
    "\n",
    "The difference between text-based question answering and visual question answering is that an image is used as context and the knowledge base.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/222095118-3d5826e4-2662-4d1c-abf2-a515f23d6d6a.png)\n",
    "\n",
    "Answering arbitrary questions about images is a complex problem because it requires involving a lot of computer vision sub-tasks. In the table below, you can find an example of questions and the required computer vision skills to find answers.\n",
    "\n",
    "| Computer vision task                   | Question examples                                       |\n",
    "|----------------------------------------| ------------------------------------------------------- |\n",
    "| Object recognition                     | What is shown in the picture? What is it?               |\n",
    "| Object detection                       | Is there any object (dog, man, book) in the image? Where is … located? |\n",
    "| Object and image attribute recognition | What color is an umbrella? Does this man wear glasses? Is there color in the image? |\n",
    "| Scene recognition                      | Is it rainy? What celebration is pictured? |\n",
    "| Object counting                        | How many players are there on the football field? How many steps are there on the stairs? |\n",
    "| Activity recognition                   | Is the baby crying? What is the woman cooking? What are they doing?                       |\n",
    "| Spatial relationships among objects    | What is located between the sofa and the armchair? What is in the bottom left corner? |\n",
    "| Commonsense reasoning                  | Does she have 100% vision? Does this person have children? |\n",
    "| Knowledge-based reasoning              | Is it a vegetarian pizza? |\n",
    "| Text recognition                       | What is the title of the book? What is shown on the screen? |\n",
    "\n",
    "\n",
    "There are a lot of applications for visual question answering:\n",
    "\n",
    "* Aid Visually Impaired Persons: VQA models can be used to reduce barriers for visually impaired people by helping them get information about images from the web and the real world.\n",
    "* Education: VQA models can be used to improve visitor experiences at museums by enabling observers to directly ask questions they are interested in or to bring more interactivity to schoolbooks for children interested in acquiring specific knowledge.\n",
    "* E-commerce: VQA models can retrieve information about products using photos from online stores.\n",
    "* Independent expert assessment: VQA models can be provide objective assessments in sports competitions, medical diagnosis, and forensic examination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a854e4-c039-418c-a716-ffaf13fb8e1d",
   "metadata": {},
   "source": [
    "## Instantiate Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The BLIP model was proposed in the [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) paper.\n",
    "\n",
    "![blip.gif](https://github.com/salesforce/BLIP/raw/main/BLIP.gif)\n",
    "\n",
    "To pre-train a unified vision-language model with both understanding and generation capabilities, BLIP introduces a multimodal mixture of an encoder-decoder and a multi-task model which can operate in one of the three modes:\n",
    "\n",
    "* **Unimodal encoders**, which separately encode images and text. The image encoder is a vision transformer. The text encoder is the same as BERT.\n",
    "* **Image-grounded text encoder**, which injects visual information by inserting a cross-attention layer between the self-attention layer and the feed-forward network for each transformer block of the text encoder.\n",
    "* **Image-grounded text decoder**, which replaces the bi-directional self-attention layers in the text encoder with causal self-attention layers.\n",
    "\n",
    "More details about the model can be found in the [research paper](https://arxiv.org/abs/2201.12086), [Salesforce blog](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/), [GitHub repo](https://github.com/salesforce/BLIP) and [Hugging Face model documentation](https://huggingface.co/docs/transformers/model_doc/blip).\n",
    "\n",
    "In this tutorial, you will use the [`blip-vqa-base`](https://huggingface.co/Salesforce/blip-vqa-base) model available for download from [Hugging Face](https://huggingface.co/). The same actions are also applicable to other similar models from the BLIP family. Although this model class is designed to perform question answering, its components can also be reused for image captioning.\n",
    "\n",
    "To start working with the model, you need to instantiate the `BlipForQuestionAnswering` class, using `from_pretrained` method. `BlipProcessor` is a helper class for preparing input data for both text and vision modalities and postprocessing of generation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "799c94e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Requested tokenizers>=0.10.1 from https://files.pythonhosted.org/packages/ff/c6/ba47d30fa820eb032a7aa6811e7bc6e0e197c012f64b544af546cce28c3d/tokenizers-0.13.4rc3.tar.gz#sha256=cf8299e33feea7fecfde53fd6d5ef39b75c118d22fb1970b3068d8daf37934e5, but installing version 0.13.4rc3\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\qsf98\\anaconda\\envs\\ncs\\python.exe' 'C:\\Users\\qsf98\\anaconda\\envs\\ncs\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\qsf98\\AppData\\Local\\Temp\\tmp6y3ubapb'\n",
      "       cwd: C:\\Users\\qsf98\\AppData\\Local\\Temp\\pip-install-a1c9jfjx\\tokenizers_f85e24ea3af046c4a739e65aaf55a9eb\n",
      "  Complete output (53 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.6\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  C:\\Users\\qsf98\\AppData\\Local\\Temp\\pip-build-env-1hnwf715\\overlay\\Lib\\site-packages\\setuptools\\dist.py:493: UserWarning: Normalizing '0.13.4.rc3' to '0.13.4rc3'\n",
      "    warnings.warn(tmpl.format(**locals()))\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --no-cache-dir --pre \"tokenizers>=0.10.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b65d89-4bd8-452b-8c2c-7dc41172a3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\qsf98\\anaconda\\envs\\ncs\\python.exe' 'C:\\Users\\qsf98\\anaconda\\envs\\ncs\\lib\\site-packages\\pip\\_vendor\\pep517\\in_process\\_in_process.py' build_wheel 'C:\\Users\\qsf98\\AppData\\Local\\Temp\\tmppiz4y5u2'\n",
      "       cwd: C:\\Users\\qsf98\\AppData\\Local\\Temp\\pip-install-zfb_9yb3\\tokenizers_4d6a496743fd40e693bfb3405d865ede\n",
      "  Complete output (51 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.6\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-3.6\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-3.6\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu torch torchvision \"transformers>=4.18.0\" gradio \"openvino>=2022.2.0\" matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d948e8c-172a-465a-a56a-c1a772866608",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-38f216bae3e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBlipProcessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBlipForQuestionAnswering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../utils\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForQuestionAnswering\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file\n",
    "\n",
    "# get model and processor\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n",
    "\n",
    "# setup test input: download and read image, prepare question\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "download_file(img_url, \"demo.jpg\")\n",
    "raw_image = Image.open(\"demo.jpg\").convert('RGB')\n",
    "question = \"how many dogs are in the picture?\"\n",
    "# preprocess input data\n",
    "inputs = processor(raw_image, question, return_tensors=\"pt\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "# perform generation\n",
    "out = model.generate(**inputs)\n",
    "end = time.perf_counter() - start\n",
    "\n",
    "# postprocess result\n",
    "answer = processor.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e53fbe-b175-4a9e-9e1b-fe6b6ae61d09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Processing time: {end:.4f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b38539-6df8-4bd6-872c-fb29b3ab6e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import visualize_results\n",
    "\n",
    "fig = visualize_results(raw_image, answer, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d17a378-d2f5-4046-a731-14f8e1983724",
   "metadata": {},
   "source": [
    "## Convert Models to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Starting from OpenVINO 2023.0 release, OpenVINO supports direct PyTorch models conversion to OpenVINO Intermediate Representation (IR) format to take the advantage of advanced OpenVINO optimization tools and features. You need to provide a model object, input data for model tracing to OpenVINO Model Conversion API. `ov.convert_model` function convert PyTorch model instance to `ov.Model` object that can be used for compilation on device or saved on disk using `ov.save_model` in compressed to FP16 format.\n",
    "\n",
    "The model consists of three parts:\n",
    "\n",
    "* vision_model - an encoder for image representation.\n",
    "* text_encoder - an encoder for input query, used for question answering and text-to-image retrieval only.\n",
    "* text_decoder - a decoder for output answer.\n",
    "\n",
    "To be able to perform multiple tasks, using the same model components, you should convert each part independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475b003-0a81-440e-8c34-6d0a3983858b",
   "metadata": {},
   "source": [
    "### Vision Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The vision model accepts float input tensors with the [1,3,384,384] shape, containing RGB image pixel values normalized in the [0,1] range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d171e085-7939-45f4-8136-a9b910cce688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import openvino as ov\n",
    "\n",
    "VISION_MODEL_OV = Path(\"blip_vision_model.xml\")\n",
    "vision_model = model.vision_model\n",
    "vision_model.eval()\n",
    "\n",
    "# check that model works and save it outputs for reusage as text encoder input\n",
    "with torch.no_grad():\n",
    "    vision_outputs = vision_model(inputs[\"pixel_values\"])\n",
    "\n",
    "# if openvino model does not exist, convert it to IR\n",
    "if not VISION_MODEL_OV.exists():\n",
    "    \n",
    "    # export pytorch model to ov.Model\n",
    "    with torch.no_grad():\n",
    "        ov_vision_model = ov.convert_model(vision_model, example_input=inputs[\"pixel_values\"])\n",
    "    # save model on disk for next usages\n",
    "    ov.save_model(ov_vision_model, VISION_MODEL_OV)\n",
    "    print(f\"Vision model successfuly converted and saved to {VISION_MODEL_OV}\")\n",
    "else:\n",
    "    print(f\"Vision model will be loaded from {VISION_MODEL_OV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e7716-f7c8-4528-8b12-fcb6b977d7ea",
   "metadata": {},
   "source": [
    "### Text Encoder\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The text encoder is used by visual question answering tasks to build a question embedding representation. It takes `input_ids` with a tokenized question and output image embeddings obtained from the vision model and attention masks for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04932e37-0138-4390-8e0a-8b70ca589dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_ENCODER_OV = Path(\"blip_text_encoder.xml\")\n",
    "\n",
    "\n",
    "text_encoder = model.text_encoder\n",
    "text_encoder.eval()\n",
    "\n",
    "# if openvino model does not exist, convert it to IR\n",
    "if not TEXT_ENCODER_OV.exists():\n",
    "    # prepare example inputs\n",
    "    image_embeds = vision_outputs[0]\n",
    "    image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n",
    "    input_dict = {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\": image_attention_mask}\n",
    "    # export PyTorch model\n",
    "    with torch.no_grad():\n",
    "        ov_text_encoder = ov.convert_model(text_encoder, example_input=input_dict)\n",
    "    # save model on disk for next usages\n",
    "    ov.save_model(ov_text_encoder, TEXT_ENCODER_OV)\n",
    "    print(f\"Text encoder successfuly converted and saved to {TEXT_ENCODER_OV}\")\n",
    "else:\n",
    "    print(f\"Text encoder will be loaded from {TEXT_ENCODER_OV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349069a-2918-4046-8c29-38a9d4b08fc8",
   "metadata": {},
   "source": [
    "### Text Decoder\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The text decoder is responsible for generating the sequence of tokens to represent model output (answer to question or caption), using an image (and question, if required) representation. The generation approach is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions. In other words, model predicts the next token in the loop guided by previously generated tokens until the stop-condition will be not reached (generated sequence of maximum length or end of string token obtained). The way the next token will be selected over predicted probabilities is driven by the selected decoding methodology. You can find more information about the most popular decoding methods in this [blog](https://huggingface.co/blog/how-to-generate). The entry point for the generation process for models from the Hugging Face Transformers library is the `generate` method. You can find more information about its parameters and configuration in the [documentation](https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/text_generation#transformers.GenerationMixin.generate). To preserve flexibility in the selection decoding methodology, you will convert only model inference for one step.\n",
    "\n",
    "To optimize the generation process and use memory more efficiently, the `use_cache=True` option is enabled. Since the output side is auto-regressive, an output token hidden state remains the same once computed for every further generation step. Therefore, recomputing it every time you want to generate a new token seems wasteful. With the cache, the model saves the hidden state once it has been computed. The model only computes the one for the most recently generated output token at each time step, re-using the saved ones for hidden tokens. This reduces the generation complexity from O(n^3) to O(n^2) for a transformer model. More details about how it works can be found in this [article](https://scale.com/blog/pytorch-improvements#Text%20Translation). With this option, the model gets the previous step's hidden states as input and additionally provides hidden states for the current step as output. Initially, you have no previous step hidden states, so the first step does not require you to provide them, but we should initialize them by default values.\n",
    "In PyTorch, past hidden state outputs are represented as a list of pairs (hidden state for key, hidden state for value] for each transformer layer in the model. OpenVINO model does not support nested outputs, they will be flattened.\n",
    "\n",
    "Similar to `text_encoder`, `text_decoder` can work with input sequences of different lengths and requires preserving dynamic input shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313d37b-54ec-42c7-89cd-360cb66375f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_decoder = model.text_decoder\n",
    "text_decoder.eval()\n",
    "\n",
    "TEXT_DECODER_OV = Path(\"blip_text_decoder_with_past.xml\")\n",
    "\n",
    "# prepare example inputs\n",
    "input_ids = torch.tensor([[30522]])  # begin of sequence token id\n",
    "attention_mask = torch.tensor([[1]])  # attention mask for input_ids\n",
    "encoder_hidden_states = torch.rand((1, 10, 768))  # encoder last hidden state from text_encoder\n",
    "encoder_attention_mask = torch.ones((1, 10), dtype=torch.long)  # attention mask for encoder hidden states\n",
    "\n",
    "input_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"encoder_hidden_states\": encoder_hidden_states, \"encoder_attention_mask\": encoder_attention_mask}\n",
    "text_decoder_outs = text_decoder(**input_dict)\n",
    "# extend input dictionary with hidden states from previous step\n",
    "input_dict[\"past_key_values\"] = text_decoder_outs[\"past_key_values\"]\n",
    "\n",
    "text_decoder.config.torchscript = True\n",
    "if not TEXT_DECODER_OV.exists():\n",
    "    # export PyTorch model\n",
    "    with torch.no_grad():\n",
    "        ov_text_decoder = ov.convert_model(text_decoder, example_input=input_dict)\n",
    "    # save model on disk for next usages\n",
    "    ov.save_model(ov_text_decoder, TEXT_DECODER_OV)\n",
    "    print(f\"Text decoder successfuly converted and saved to {TEXT_DECODER_OV}\")\n",
    "else:\n",
    "    print(f\"Text decoder will be loaded from {TEXT_DECODER_OV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27838599-dc4e-4042-8eed-447671a1fae5",
   "metadata": {},
   "source": [
    "## Run OpenVINO Model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Prepare Inference Pipeline\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "As discussed before, the model consists of several blocks which can be reused for building pipelines for different tasks. In the diagram below, you can see how image captioning works:\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/221865836-a56da06e-196d-449c-a5dc-4136da6ab5d5.png)\n",
    "\n",
    "\n",
    "The visual model accepts the image preprocessed by `BlipProcessor` as input and produces image embeddings, which are directly passed to the text decoder for generation caption tokens. When generation is finished, output sequence of tokens is provided to `BlipProcessor` for decoding to text using a tokenizer.\n",
    "\n",
    "The pipeline for question answering looks similar, but with additional question processing. In this case, image embeddings and question tokenized by `BlipProcessor` are provided to the text encoder and then multimodal question embedding is passed to the text decoder for performing generation of answers.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/29454499/221868167-d0081add-d9f3-4591-80e7-4753c88c1d0a.png)\n",
    "\n",
    "The next step is implementing both pipelines using OpenVINO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735eacb0-6639-4a20-ab22-cd706f38b473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create OpenVINO Core object instance\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e43838-6986-4199-b029-f18ee94e0cb4",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918cbf42-a4eb-433d-af3c-245251729173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8380db0-0f5a-4234-b92f-cd7ea01d774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models on device\n",
    "ov_vision_model = core.compile_model(VISION_MODEL_OV, device.value)\n",
    "ov_text_encoder = core.compile_model(TEXT_ENCODER_OV, device.value)\n",
    "ov_text_decoder_with_past = core.compile_model(TEXT_DECODER_OV, device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bfc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from blip_model import text_decoder_forward\n",
    "\n",
    "text_decoder.forward = partial(text_decoder_forward, ov_text_decoder_with_past=ov_text_decoder_with_past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1ceee-24fe-4247-9328-640adb2e7019",
   "metadata": {},
   "source": [
    "The model helper class has two methods for generation: **generate_answer** - used for visual question answering, **generate_caption** - used for caption generation.\n",
    "For initialization, model class accepts compiled OpenVINO models for the text encoder, vision model and text decoder, and also configuration for generation and initial token for decoder work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba0c67-cc02-4781-bb3a-472146362a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from blip_model import OVBlipModel\n",
    "\n",
    "ov_model = OVBlipModel(model.config, model.decoder_start_token_id, ov_vision_model, ov_text_encoder, text_decoder)\n",
    "out = ov_model.generate_answer(**inputs, max_length=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a8ddc",
   "metadata": {},
   "source": [
    "Now, the model is ready for generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea557140-a8bb-4ce8-9981-cf66abc62e87",
   "metadata": {},
   "source": [
    "### Image Captioning\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c522e8-94df-45fc-b0e0-2b3b02b44f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = ov_model.generate_caption(inputs[\"pixel_values\"], max_length=20)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "fig = visualize_results(raw_image, caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37484aea-4b2c-4e6d-907e-020230a0db9e",
   "metadata": {},
   "source": [
    "### Question Answering\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af014362-6c09-4802-848a-37d8726c8110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "out = ov_model.generate_answer(**inputs, max_length=20)\n",
    "end = time.perf_counter() - start\n",
    "answer = processor.decode(out[0], skip_special_tokens=True)\n",
    "fig = visualize_results(raw_image, answer, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7804928-481d-4867-9d0e-2043bf2031b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Processing time: {end:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660d6bb-a9b2-48f6-880a-6f2885b978b6",
   "metadata": {},
   "source": [
    "## Interactive demo\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1aeca-f6bc-4eef-b51a-cd65609be7aa",
   "metadata": {
    "test_replace": {
     "    demo.launch(debug=True)": "    demo.launch()",
     "    demo.launch(share=True, debug=True)": "    demo.launch(share=True)"
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_answer(img, question):\n",
    "    if img is None:\n",
    "        raise gr.Error(\"Please upload an image or choose one from the examples list\")\n",
    "    start = time.perf_counter()\n",
    "    inputs = processor(img, question, return_tensors=\"pt\")\n",
    "    output = (\n",
    "        ov_model.generate_answer(**inputs, max_length=20)\n",
    "        if len(question)\n",
    "        else ov_model.generate_caption(inputs[\"pixel_values\"], max_length=20)\n",
    "    )\n",
    "    answer = processor.decode(output[0], skip_special_tokens=True)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    html = f\"<p>Processing time: {elapsed:.4f}</p>\"\n",
    "    return answer, html\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate_answer,\n",
    "    [\n",
    "        gr.Image(label=\"Image\"),\n",
    "        gr.Textbox(\n",
    "            label=\"Question\",\n",
    "            info=\"If this field is empty, an image caption will be generated\",\n",
    "        ),\n",
    "    ],\n",
    "    [gr.Text(label=\"Answer\"), gr.HTML()],\n",
    "    examples=[[\"demo.jpg\", \"\"], [\"demo.jpg\", question]],\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(share=True, debug=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d11f7e",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Open the [233-blip-optimize](233-blip-optimize.ipynb) notebook to quantize vision and text encoder models with the Post-training Quantization API of NNCF\n",
    "and compress weights of the text decoder. Then compare the converted and optimized OpenVINO models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "30f6166f5f0cb6253cad15b1c8ca46093b160f1914c051aeccf8063f98b299b9"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
